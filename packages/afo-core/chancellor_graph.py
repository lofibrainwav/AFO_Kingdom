from typing import Annotated, Any, TypedDict

from langchain_core.messages import AIMessage, BaseMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages

# Import existing LLM Router logic for model execution
from llm_router import QualityTier, llm_router

# Antigravity í†µí•© (çœ: ëª…ì‹œì  ì„¤ì • ì „ë‹¬)
try:
    from AFO.config.antigravity import antigravity
except ImportError:
    try:
        from config.antigravity import antigravity
    except ImportError:
        # Fallback: ê¸°ë³¸ê°’ ì‚¬ìš©
        class MockAntigravity:
            AUTO_DEPLOY = True
            DRY_RUN_DEFAULT = True
            ENVIRONMENT = "dev"
        antigravity = MockAntigravity()


# --- 1. State Definition (Chancellor's Memory - V2 Constitution) ---
class ChancellorState(TypedDict):
    # 1. çœ (Truth): Persistent Message History (Auto-merge)
    messages: Annotated[list[BaseMessage], add_messages]

    # 2. çœ/å–„ (Metrics): Decision Basis
    trinity_score: float  # Current Trinity Score
    risk_score: float  # Current Risk Score

    # 3. å­ (Serenity): Auto-Run Eligibility
    auto_run_eligible: bool  # If True, bypass human approval

    # 4. å¤© (Context): External Environment
    kingdom_context: dict[str, Any]  # e.g. Family status, verification results

    # 5. æ°¸ (Memory): Long-term Memory
    persistent_memory: dict[str, Any]

    # Operational fields
    current_speaker: str  # "user", "chancellor", "jegalryang", "samaui", "juyu"
    next_step: str  # Next node to visit
    analysis_results: dict[str, str]  # Store individual strategist outputs


# --- 2. Node Definitions (The Personas) ---


def chancellor_router_node(state: ChancellorState):
    """
    [Chancellor Node]
    The Supreme Orchestrator.
    Decides which Strategist should speak next or if the final answer is ready.
    
    Antigravity í†µí•©: DRY_RUN ëª¨ë“œ ê°ì§€ ë° auto_run_eligible ì¡°ì •
    """
    print("ğŸ‘‘ [Chancellor] Analyzing state...")
    messages = state["messages"]
    messages[-1]

    # Antigravity ì„¤ì • í™•ì¸ (kingdom_contextì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì „ì—­ ì„¤ì • ì‚¬ìš©)
    context = state.get("kingdom_context", {}) or {}
    antigravity_config = context.get("antigravity", {})
    is_dry_run = antigravity_config.get("DRY_RUN_DEFAULT", antigravity.DRY_RUN_DEFAULT)
    
    # DRY_RUN ëª¨ë“œì¼ ë•ŒëŠ” auto_run_eligibleì„ Falseë¡œ ê°•ì œ (å–„: ì•ˆì „ ìš°ì„ )
    if is_dry_run and state.get("auto_run_eligible", False):
        print("ğŸ›¡ï¸ [Chancellor] DRY_RUN ëª¨ë“œ ê°ì§€ - auto_run_eligibleì„ Falseë¡œ ì¡°ì • (å–„)")
        state["auto_run_eligible"] = False

    # Simple heuristic routing for now (Upgrade to LLM-based later)
    # If this is the start (Human message), we might want to trigger all three or specific ones.
    # For V1, let's trigger Jegalryang (Truth) first if no analysis exists.

    analysis = state.get("analysis_results", {})

    strategist_order = context.get("strategist_order") or ["jegalryang", "samaui", "juyu"]
    max_strategists = context.get("max_strategists")
    try:
        max_n = int(max_strategists) if max_strategists is not None else len(strategist_order)
    except Exception:
        max_n = len(strategist_order)

    strategist_order = list(strategist_order)[: max(0, min(3, max_n))]

    # Visit strategists in requested order, up to max_strategists.
    for strategist in strategist_order:
        if strategist not in analysis:
            return {"next_step": strategist, "current_speaker": "chancellor"}

    # If all requested strategists have spoken, Chancellor synthesizes the final answer.
    return {"next_step": "finalize", "current_speaker": "chancellor"}


async def jegalryang_node(state: ChancellorState):
    """
    [Jegalryang Node] - Truth (çŸ›)
    Focus: Architecture, Strategy, Technical Certainty.
    """
    print("âš”ï¸ [Jegalryang] Analyzing Truth...")
    query = state["messages"][-1].content

    # Use LLM Router to call a "Smart" model (Truth requires intelligence)
    # Context can be passed to select specific persona prompts if we had them loaded here.
    # For now, we simulate the persona via system context augmentation in a real implementation.

    # In a full implementation, we would inject the System Prompt from TRINITY-OS/docs/personas/STRATEGIST_JEGALRYANG.md
    base_context = (state.get("kingdom_context") or {}).get("llm_context") or {}
    context = {
        **base_context,
        "quality_tier": base_context.get("quality_tier", QualityTier.PREMIUM),
        "max_tokens": base_context.get("max_tokens", 512),
    }

    response_data = await llm_router.execute_with_routing(
        f"ë‹¹ì‹ ì€ ì œê°ˆëŸ‰(Truth)ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ìˆ ì /êµ¬ì¡°ì  ê´€ì ì—ì„œ ë¶„ì„í•˜ì‹œì˜¤: {query}",
        context=context,
    )

    content = response_data.get("response", "ë¶„ì„ ì‹¤íŒ¨")

    return {
        "analysis_results": {**state.get("analysis_results", {}), "jegalryang": content},
        "messages": [AIMessage(content=f"[ì œê°ˆëŸ‰] {content}", name="jegalryang")],
    }


async def samaui_node(state: ChancellorState):
    """
    [Samaui Node] - Goodness (ç›¾)
    Focus: Ethics, Stability, Risk Management.
    """
    print("ğŸ›¡ï¸ [Samaui] Checking Risks...")
    query = state["messages"][0].content  # Analyze original query
    truth_analysis = state["analysis_results"].get("jegalryang", "")

    base_context = (state.get("kingdom_context") or {}).get("llm_context") or {}
    context = {
        **base_context,
        "quality_tier": base_context.get("quality_tier", QualityTier.STANDARD),
        "max_tokens": base_context.get("max_tokens", 512),
    }

    response_data = await llm_router.execute_with_routing(
        f"ë‹¹ì‹ ì€ ì‚¬ë§ˆì˜(Goodness)ì…ë‹ˆë‹¤. ì œê°ˆëŸ‰ì˜ ë¶„ì„('{truth_analysis[:200]}...')ê³¼ ì›ë³¸ ì§ˆë¬¸('{query}')ì„ ë³´ê³  ìœ¤ë¦¬ì /ì•ˆì „ ë¦¬ìŠ¤í¬ë¥¼ ê²€í† í•˜ì‹œì˜¤.",
        context=context,
    )

    content = response_data.get("response", "ê²€í†  ì‹¤íŒ¨")

    return {
        "analysis_results": {**state.get("analysis_results", {}), "samaui": content},
        "messages": [AIMessage(content=f"[ì‚¬ë§ˆì˜] {content}", name="samaui")],
    }


async def juyu_node(state: ChancellorState):
    """
    [Juyu Node] - Beauty (æ©‹)
    Focus: Narrative, UX, User Experience.
    """
    print("ğŸŒ‰ [Juyu] Polishing UX...")
    state["messages"][0].content
    truth = state["analysis_results"].get("jegalryang", "")
    goodness = state["analysis_results"].get("samaui", "")

    base_context = (state.get("kingdom_context") or {}).get("llm_context") or {}
    context = {
        **base_context,
        "quality_tier": base_context.get("quality_tier", QualityTier.PREMIUM),
        "max_tokens": base_context.get("max_tokens", 512),
    }

    response_data = await llm_router.execute_with_routing(
        f"ë‹¹ì‹ ì€ ì£¼ìœ (Beauty)ì…ë‹ˆë‹¤. ê¸°ìˆ ({truth[:100]}...)ê³¼ ì•ˆì „({goodness[:100]}...)ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì•„ë¦„ë‹µê³  ì‰¬ìš´ ì„œì‚¬ë¡œ ì •ë¦¬í•˜ì‹œì˜¤.",
        context=context,
    )

    content = response_data.get("response", "ì •ë¦¬ ì‹¤íŒ¨")

    return {
        "analysis_results": {**state.get("analysis_results", {}), "juyu": content},
        "messages": [AIMessage(content=f"[ì£¼ìœ ] {content}", name="juyu")],
    }


async def chancellor_finalize_node(state: ChancellorState):
    """
    [Finalize]
    Chancellor synthesizes the final report.
    """
    print("ğŸ‘‘ [Chancellor] Synthesizing Final Report...")
    analysis = state["analysis_results"]

    # Only include available analyses (graph may be configured to consult fewer strategists).
    parts: list[str] = [
        "ë‹¹ì‹ ì€ ìŠ¹ìƒ(Chancellor)ì…ë‹ˆë‹¤. ì±…ì‚¬ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ë¥¼ í•˜ì‹œì˜¤.",
        "ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ì‚¬ë ¹ê´€ì˜ í‰ì˜¨(å­)ì…ë‹ˆë‹¤.",
    ]
    if analysis.get("jegalryang"):
        parts.append(f"[ì œê°ˆëŸ‰]: {analysis.get('jegalryang')}")
    if analysis.get("samaui"):
        parts.append(f"[ì‚¬ë§ˆì˜]: {analysis.get('samaui')}")
    if analysis.get("juyu"):
        parts.append(f"[ì£¼ìœ ]: {analysis.get('juyu')}")
    final_prompt = "\n\n".join(parts)

    base_context = (state.get("kingdom_context") or {}).get("llm_context") or {}
    context = {
        **base_context,
        "quality_tier": base_context.get("quality_tier", QualityTier.ULTRA),
        "max_tokens": base_context.get("max_tokens", 768),
    }

    response_data = await llm_router.execute_with_routing(final_prompt, context=context)

    content = response_data.get("response", "ì¢…í•© ì‹¤íŒ¨")

    return {"messages": [AIMessage(content=content, name="chancellor")]}


# --- 3. Graph Construction ---


def build_chancellor_graph():
    workflow = StateGraph(ChancellorState)

    # Add Nodes
    workflow.add_node("chancellor", chancellor_router_node)
    workflow.add_node("jegalryang", jegalryang_node)
    workflow.add_node("samaui", samaui_node)
    workflow.add_node("juyu", juyu_node)
    workflow.add_node("finalize", chancellor_finalize_node)

    # Add Edges
    workflow.set_entry_point("chancellor")

    # Conditional Edge from Chancellor
    def route_logic(state):
        return state["next_step"]

    workflow.add_conditional_edges(
        "chancellor",
        route_logic,
        {"jegalryang": "jegalryang", "samaui": "samaui", "juyu": "juyu", "finalize": "finalize"},
    )

    # Strategies return to Chancellor
    workflow.add_edge("jegalryang", "chancellor")
    workflow.add_edge("samaui", "chancellor")
    workflow.add_edge("juyu", "chancellor")
    workflow.add_edge("finalize", END)

    # Persistence Strategy (Dev: Memory, Prod: Postgres)
    # Using MemorySaver for current verification as per V2 Constitution (Dev Mode)
    checkpointer = MemorySaver()

    return workflow.compile(checkpointer=checkpointer)


# Singleton Instance
chancellor_graph = build_chancellor_graph()
