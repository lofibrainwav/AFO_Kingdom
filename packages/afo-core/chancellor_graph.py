from typing import Annotated, Any, TypedDict

from langchain_core.messages import AIMessage, BaseMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages

# Import existing LLM Router logic for model execution
from llm_router import QualityTier, llm_router


# --- 1. State Definition (Chancellor's Memory - V2 Constitution) ---
class ChancellorState(TypedDict):
    # 1. çœ (Truth): Persistent Message History (Auto-merge)
    messages: Annotated[list[BaseMessage], add_messages]

    # 2. çœ/å–„ (Metrics): Decision Basis
    trinity_score: float  # Current Trinity Score
    risk_score: float  # Current Risk Score

    # 3. å­ (Serenity): Auto-Run Eligibility
    auto_run_eligible: bool  # If True, bypass human approval

    # 4. å¤© (Context): External Environment
    kingdom_context: dict[str, Any]  # e.g. Family status, verification results

    # 5. æ°¸ (Memory): Long-term Memory
    persistent_memory: dict[str, Any]

    # Operational fields
    current_speaker: str  # "user", "chancellor", "jegalryang", "samaui", "juyu"
    next_step: str  # Next node to visit
    analysis_results: dict[str, str]  # Store individual strategist outputs


# --- 2. Node Definitions (The Personas) ---


def chancellor_router_node(state: ChancellorState):
    """
    [Chancellor Node]
    The Supreme Orchestrator.
    Decides which Strategist should speak next or if the final answer is ready.
    """
    print("ğŸ‘‘ [Chancellor] Analyzing state...")
    messages = state["messages"]
    messages[-1]

    # Simple heuristic routing for now (Upgrade to LLM-based later)
    # If this is the start (Human message), we might want to trigger all three or specific ones.
    # For V1, let's trigger Jegalryang (Truth) first if no analysis exists.

    analysis = state.get("analysis_results", {})

    # 1. If no Truth analysis, call Jegalryang
    if "jegalryang" not in analysis:
        return {"next_step": "jegalryang", "current_speaker": "chancellor"}

    # 2. If Truth exists but no Goodness, call Samaui
    if "samaui" not in analysis:
        return {"next_step": "samaui", "current_speaker": "chancellor"}

    # 3. If Truth & Goodness exist but no Beauty, call Juyu
    if "juyu" not in analysis:
        return {"next_step": "juyu", "current_speaker": "chancellor"}

    # 4. If all three have spoken, Chancellor synthesizes the final answer.
    return {"next_step": "finalize", "current_speaker": "chancellor"}


async def jegalryang_node(state: ChancellorState):
    """
    [Jegalryang Node] - Truth (çŸ›)
    Focus: Architecture, Strategy, Technical Certainty.
    """
    print("âš”ï¸ [Jegalryang] Analyzing Truth...")
    query = state["messages"][-1].content

    # Use LLM Router to call a "Smart" model (Truth requires intelligence)
    # Context can be passed to select specific persona prompts if we had them loaded here.
    # For now, we simulate the persona via system context augmentation in a real implementation.

    # In a full implementation, we would inject the System Prompt from TRINITY-OS/docs/personas/STRATEGIST_JEGALRYANG.md
    response_data = await llm_router.execute_with_routing(
        f"ë‹¹ì‹ ì€ ì œê°ˆëŸ‰(Truth)ì…ë‹ˆë‹¤. ë‹¤ìŒ ì§ˆë¬¸ì„ ê¸°ìˆ ì /êµ¬ì¡°ì  ê´€ì ì—ì„œ ë¶„ì„í•˜ì‹œì˜¤: {query}",
        context={"quality_tier": QualityTier.PREMIUM},
    )

    content = response_data.get("response", "ë¶„ì„ ì‹¤íŒ¨")

    return {
        "analysis_results": {**state.get("analysis_results", {}), "jegalryang": content},
        "messages": [AIMessage(content=f"[ì œê°ˆëŸ‰] {content}", name="jegalryang")],
    }


async def samaui_node(state: ChancellorState):
    """
    [Samaui Node] - Goodness (ç›¾)
    Focus: Ethics, Stability, Risk Management.
    """
    print("ğŸ›¡ï¸ [Samaui] Checking Risks...")
    query = state["messages"][0].content  # Analyze original query
    truth_analysis = state["analysis_results"].get("jegalryang", "")

    response_data = await llm_router.execute_with_routing(
        f"ë‹¹ì‹ ì€ ì‚¬ë§ˆì˜(Goodness)ì…ë‹ˆë‹¤. ì œê°ˆëŸ‰ì˜ ë¶„ì„('{truth_analysis[:200]}...')ê³¼ ì›ë³¸ ì§ˆë¬¸('{query}')ì„ ë³´ê³  ìœ¤ë¦¬ì /ì•ˆì „ ë¦¬ìŠ¤í¬ë¥¼ ê²€í† í•˜ì‹œì˜¤.",
        context={"quality_tier": QualityTier.STANDARD},
    )

    content = response_data.get("response", "ê²€í†  ì‹¤íŒ¨")

    return {
        "analysis_results": {**state.get("analysis_results", {}), "samaui": content},
        "messages": [AIMessage(content=f"[ì‚¬ë§ˆì˜] {content}", name="samaui")],
    }


async def juyu_node(state: ChancellorState):
    """
    [Juyu Node] - Beauty (æ©‹)
    Focus: Narrative, UX, User Experience.
    """
    print("ğŸŒ‰ [Juyu] Polishing UX...")
    state["messages"][0].content
    truth = state["analysis_results"].get("jegalryang", "")
    goodness = state["analysis_results"].get("samaui", "")

    response_data = await llm_router.execute_with_routing(
        f"ë‹¹ì‹ ì€ ì£¼ìœ (Beauty)ì…ë‹ˆë‹¤. ê¸°ìˆ ({truth[:100]}...)ê³¼ ì•ˆì „({goodness[:100]}...)ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì•„ë¦„ë‹µê³  ì‰¬ìš´ ì„œì‚¬ë¡œ ì •ë¦¬í•˜ì‹œì˜¤.",
        context={"quality_tier": QualityTier.PREMIUM},
    )

    content = response_data.get("response", "ì •ë¦¬ ì‹¤íŒ¨")

    return {
        "analysis_results": {**state.get("analysis_results", {}), "juyu": content},
        "messages": [AIMessage(content=f"[ì£¼ìœ ] {content}", name="juyu")],
    }


async def chancellor_finalize_node(state: ChancellorState):
    """
    [Finalize]
    Chancellor synthesizes the final report.
    """
    print("ğŸ‘‘ [Chancellor] Synthesizing Final Report...")
    analysis = state["analysis_results"]

    final_prompt = f"""
    ë‹¹ì‹ ì€ ìŠ¹ìƒ(Chancellor)ì…ë‹ˆë‹¤. 3ì±…ì‚¬ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ë¥¼ í•˜ì‹œì˜¤.
    ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ì‚¬ë ¹ê´€ì˜ í‰ì˜¨(å­)ì…ë‹ˆë‹¤.

    [ì œê°ˆëŸ‰]: {analysis.get("jegalryang")}
    [ì‚¬ë§ˆì˜]: {analysis.get("samaui")}
    [ì£¼ìœ ]: {analysis.get("juyu")}
    """

    response_data = await llm_router.execute_with_routing(
        final_prompt, context={"quality_tier": QualityTier.ULTRA}
    )

    content = response_data.get("response", "ì¢…í•© ì‹¤íŒ¨")

    return {"messages": [AIMessage(content=content, name="chancellor")]}


# --- 3. Graph Construction ---


def build_chancellor_graph():
    workflow = StateGraph(ChancellorState)

    # Add Nodes
    workflow.add_node("chancellor", chancellor_router_node)
    workflow.add_node("jegalryang", jegalryang_node)
    workflow.add_node("samaui", samaui_node)
    workflow.add_node("juyu", juyu_node)
    workflow.add_node("finalize", chancellor_finalize_node)

    # Add Edges
    workflow.set_entry_point("chancellor")

    # Conditional Edge from Chancellor
    def route_logic(state):
        return state["next_step"]

    workflow.add_conditional_edges(
        "chancellor",
        route_logic,
        {"jegalryang": "jegalryang", "samaui": "samaui", "juyu": "juyu", "finalize": "finalize"},
    )

    # Strategies return to Chancellor
    workflow.add_edge("jegalryang", "chancellor")
    workflow.add_edge("samaui", "chancellor")
    workflow.add_edge("juyu", "chancellor")
    workflow.add_edge("finalize", END)

    # Persistence Strategy (Dev: Memory, Prod: Postgres)
    # Using MemorySaver for current verification as per V2 Constitution (Dev Mode)
    checkpointer = MemorySaver()

    return workflow.compile(checkpointer=checkpointer)


# Singleton Instance
chancellor_graph = build_chancellor_graph()
