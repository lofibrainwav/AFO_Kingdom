"""
AFO LLM Router - í•˜ì´ë¸Œë¦¬ë“œ ë©€í‹°-LLM í†µí•© ì‹œìŠ¤í…œ
Ollama (ë‚´ë¶€ ì§€ë ¥) + API LLM (ì™¸ë¶€ ë¬´ë ¥) ê°„ ë™ì  ë¼ìš°íŒ…

ê¸°ëŠ¥:
- Latency ê¸°ë°˜ ìë™ fallback
- Cost ìµœì í™” ë¼ìš°íŒ…
- Quality ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ì„ íƒ
- Graceful degradation
"""

from __future__ import annotations

import asyncio
import contextlib
import logging
import os
import time
from collections import OrderedDict
from dataclasses import dataclass
from enum import Enum
from typing import Any

# API wrapper imports
# [ì¥ì] ë¬´ìš©ì§€ìš© - ë¶ˆí•„ìš”í•œ ì£¼ì„ì€ ì œê±°í•˜ì—¬ ì§„ì‹¤ì„ ë“œëŸ¬ëƒ„
try:
    from AFO.llms.claude_api import claude_api
    from AFO.llms.gemini_api import gemini_api
    from AFO.llms.openai_api import openai_api

    API_WRAPPERS_AVAILABLE = True
except ImportError as e:
    API_WRAPPERS_AVAILABLE = False
    logging.warning(f"âš ï¸ API wrappers not available: {e}")

logger = logging.getLogger(__name__)


class LLMProvider(Enum):
    """LLM ì œê³µì íƒ€ì…"""

    OLLAMA = "ollama"
    ANTHROPIC = "anthropic"
    GEMINI = "gemini"
    OPENAI = "openai"


class QualityTier(Enum):
    """í’ˆì§ˆ ë“±ê¸‰"""

    BASIC = "basic"  # ê¸°ë³¸ ì‘ë‹µ
    STANDARD = "standard"  # í‘œì¤€ í’ˆì§ˆ
    PREMIUM = "premium"  # ê³ í’ˆì§ˆ
    ULTRA = "ultra"  # ìµœê³  í’ˆì§ˆ


@dataclass
class LLMConfig:
    """LLM ì„¤ì •"""

    provider: LLMProvider
    model: str
    api_key_env: str | None = None
    base_url: str | None = None
    temperature: float = 0.7
    max_tokens: int = 1024
    cost_per_token: float = 0.0
    latency_ms: int = 1000
    quality_tier: QualityTier = QualityTier.STANDARD
    context_window: int = 4096  # Default context window


@dataclass
class RoutingDecision:
    """ë¼ìš°íŒ… ê²°ì •"""

    selected_provider: LLMProvider
    selected_model: str
    reasoning: str
    confidence: float
    estimated_cost: float
    estimated_latency: int
    fallback_providers: list[LLMProvider]


class LLMRouter:
    """
    í•˜ì´ë¸Œë¦¬ë“œ LLM ë¼ìš°í„°
    Ollama ìš°ì„  â†’ API LLM fallback
    """

    def __init__(self) -> None:
        self.llm_configs: dict[LLMProvider, LLMConfig] = {}
        self._initialize_configs()
        self.routing_history: list[dict[str, Any]] = []
        # Simple LRU Cache
        self._response_cache: OrderedDict[str, dict[str, Any]] = OrderedDict()
        self._cache_max_size = 100
        self._cache_ttl = 300  # 5 minutes

    def _initialize_configs(self) -> None:
        """LLM ì„¤ì • ì´ˆê¸°í™”"""
        # Phase 2-4: settings ì‚¬ìš©
        try:
            from AFO.config.settings import get_settings

            settings = get_settings()
        except ImportError:
            try:
                # [ë§¹ì] ë“ë„ë‹¤ì¡° = ì—¬ëŸ¬ ê¸¸ì„ ì‹œë„í•˜ì—¬ ë„ì›€ì„ êµ¬í•¨
                from config.settings import get_settings  # type: ignore[assignment]

                settings = get_settings()
            except ImportError:
                settings = None

        # Ollama (ë‚´ë¶€ ì§€ë ¥)
        ollama_model = (
            settings.OLLAMA_MODEL if settings else os.getenv("OLLAMA_MODEL", "qwen3-vl:8b")
        )
        ollama_base_url = (
            settings.OLLAMA_BASE_URL
            if settings
            else os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        )

        self.llm_configs[LLMProvider.OLLAMA] = LLMConfig(
            provider=LLMProvider.OLLAMA,
            model=ollama_model,  # Phase 2-4: settings ì‚¬ìš©
            base_url=ollama_base_url,  # Phase 2-4: settings ì‚¬ìš©
            temperature=0.7,
            max_tokens=2048,
            cost_per_token=0.0,  # ë¬´ë£Œ
            latency_ms=500,  # ë¡œì»¬ì´ë¯€ë¡œ ë¹ ë¦„
            quality_tier=QualityTier.PREMIUM,
            context_window=8192,  # Qwen3-VL-8B supports larger context
        )

        # Vault Integration - [ì†ì] ì§€í”¼ì§€ê¸° - ë¹„ë°€ì„ ì•„ëŠ” ìê°€ ìŠ¹ë¦¬í•¨
        vault: Any = None
        try:
            from AFO.security.vault_manager import vault as _vault
            vault = _vault
        except (ImportError, ValueError):
            try:
                from security.vault_manager import vault as _v2
                vault = _v2
            except ImportError:
                vault = None

        # Helper to get secret
        def get_secret(name: str) -> str | None:
            if vault:
                # Type safe call
                res = vault.get_secret(name)
                return res if isinstance(res, str) else None
            return getattr(settings, name, None) if settings else os.getenv(name)

        # Anthropic (Claude)
        anthropic_key = get_secret("ANTHROPIC_API_KEY")
        if anthropic_key:
            self.llm_configs[LLMProvider.ANTHROPIC] = LLMConfig(
                provider=LLMProvider.ANTHROPIC,
                model="claude-3-sonnet-20240229",
                api_key_env="ANTHROPIC_API_KEY",
                temperature=0.7,
                max_tokens=4096,
                cost_per_token=0.000015,
                latency_ms=2000,
                quality_tier=QualityTier.ULTRA,
            )

        # Google Gemini
        gemini_key = get_secret("GEMINI_API_KEY")
        google_key = get_secret("GOOGLE_API_KEY")
        if gemini_key or google_key:
            self.llm_configs[LLMProvider.GEMINI] = LLMConfig(
                provider=LLMProvider.GEMINI,
                model="gemini-2.0-flash-exp",  # Updated to latest model
                api_key_env="GEMINI_API_KEY" if gemini_key else "GOOGLE_API_KEY",
                temperature=0.7,
                max_tokens=2048,
                cost_per_token=0.0000005,
                latency_ms=500,
                quality_tier=QualityTier.PREMIUM,
            )

        # OpenAI GPT
        openai_key = get_secret("OPENAI_API_KEY")
        if openai_key:
            self.llm_configs[LLMProvider.OPENAI] = LLMConfig(
                provider=LLMProvider.OPENAI,
                model="gpt-4o",
                api_key_env="OPENAI_API_KEY",
                temperature=0.7,
                max_tokens=4096,
                cost_per_token=0.000005,  # ì…ë ¥ í† í°ë‹¹
                latency_ms=1800,
                quality_tier=QualityTier.ULTRA,
            )

        logger.info(f"âœ… LLM Router ì´ˆê¸°í™”: {len(self.llm_configs)}ê°œ LLM ì„¤ì •ë¨")
        # Start health check in background implies calling check_connections async,
        # but in sync __init__ we just log.

    async def check_connections(self) -> dict[str, bool]:
        """Startup Health Check for all providers"""
        results = {}
        logger.info("ğŸ¥ Running LLM Health Checks...")

        # Check Ollama
        if self._is_ollama_available():
            results["Ollama"] = True
            logger.info("   âœ… Ollama: Online")
        else:
            results["Ollama"] = False
            logger.warning("   âŒ Ollama: Offline or Unreachable")

        return results

    def route_request(self, query: str, context: dict[str, Any] | None = None) -> RoutingDecision:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ìµœì  LLM ë¼ìš°íŒ… ê²°ì •
        """
        context = context or {}
        quality_requirement = context.get("quality_tier", QualityTier.STANDARD)
        latency_requirement = context.get("max_latency_ms", 5000)
        cost_budget = context.get("max_cost", 1.0)

        # 0. ëª…ì‹œì  í”„ë¡œë°”ì´ë” ìš”ì²­ ì²˜ë¦¬
        requested_provider = context.get("provider")
        if requested_provider and requested_provider != "auto":
            try:
                target_provider = LLMProvider(requested_provider)
                config = self.llm_configs.get(target_provider)

                if config:
                    return RoutingDecision(
                        selected_provider=target_provider,
                        selected_model=config.model,
                        reasoning=f"ì‚¬ìš©ì ëª…ì‹œì  ìš”ì²­: {requested_provider}",
                        confidence=1.0,
                        estimated_cost=self._estimate_cost(query, config),
                        estimated_latency=config.latency_ms,
                        fallback_providers=[LLMProvider.OLLAMA],
                    )
            except ValueError:
                pass  # ìœ íš¨í•˜ì§€ ì•Šì€ provider ë¬¸ìì—´ì€ ë¬´ì‹œ

        # 1. Ollama ìš°ì„  ì„ íƒ (ë‚´ë¶€ ì§€ë ¥)
        if self._is_ollama_available():
            decision = RoutingDecision(
                selected_provider=LLMProvider.OLLAMA,
                selected_model="qwen3-vl:8b",
                reasoning="ë‚´ë¶€ ì§€ë ¥(Ollama) ìš°ì„  ì‚¬ìš© - ë¹„ìš© ì ˆê° ë° ì†ë„ ìµœì í™”",
                confidence=0.9,
                estimated_cost=0.0,
                estimated_latency=500,
                fallback_providers=[LLMProvider.ANTHROPIC, LLMProvider.GEMINI, LLMProvider.OPENAI],
            )

            # í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ì´ ULTRAì¸ ê²½ìš° API LLMìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
            if quality_requirement == QualityTier.ULTRA and self._has_ultra_llm():
                decision = self._upgrade_to_ultra(query, context)

            return decision

        # 2. Ollama ë¶ˆê°€ ì‹œ API LLM ì„ íƒ
        return self._select_api_llm(
            query, context, quality_requirement, latency_requirement, cost_budget
        )

    def _is_ollama_available(self) -> bool:
        """Ollama ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ í—¬ìŠ¤ ì²´í¬ (ì‹¤ì œë¡œëŠ” HTTP ìš”ì²­)
            config = self.llm_configs.get(LLMProvider.OLLAMA)
            return config is not None and config.base_url is not None
        except Exception:
            return False

    def _has_ultra_llm(self) -> bool:
        """ULTRA í’ˆì§ˆ LLM ë³´ìœ  ì—¬ë¶€"""
        return any(config.quality_tier == QualityTier.ULTRA for config in self.llm_configs.values())

    def _upgrade_to_ultra(self, query: str, context: dict[str, Any]) -> RoutingDecision:
        """ULTRA í’ˆì§ˆë¡œ ì—…ê·¸ë ˆì´ë“œ"""
        # ê°€ì¥ ì €ë ´í•œ ULTRA LLM ì„ íƒ
        ultra_llms = [
            config
            for config in self.llm_configs.values()
            if config.quality_tier == QualityTier.ULTRA
        ]

        if not ultra_llms:
            return self._select_api_llm(query, context, QualityTier.ULTRA, 5000, 1.0)

        # ë¹„ìš© ê¸°ì¤€ ì •ë ¬
        best_llm = min(ultra_llms, key=lambda x: x.cost_per_token)

        return RoutingDecision(
            selected_provider=best_llm.provider,
            selected_model=best_llm.model,
            reasoning=f"ULTRA í’ˆì§ˆ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ {best_llm.provider.value}ë¡œ ì—…ê·¸ë ˆì´ë“œ",
            confidence=0.95,
            estimated_cost=self._estimate_cost(query, best_llm),
            estimated_latency=best_llm.latency_ms,
            fallback_providers=[p.provider for p in ultra_llms if p != best_llm],
        )

    def _select_api_llm(
        self,
        query: str,
        context: dict[str, Any],
        quality: QualityTier,
        max_latency: int,
        max_cost: float,
    ) -> RoutingDecision:
        """API LLM ì„ íƒ (Ollama ë¶ˆê°€ ì‹œ)"""
        candidates = [
            config
            for config in self.llm_configs.values()
            if config.provider != LLMProvider.OLLAMA
            and config.quality_tier.value >= quality.value
            and config.latency_ms <= max_latency
        ]

        if not candidates:
            # í’ˆì§ˆ ë‚®ì¶°ì„œ ì¬ì‹œë„
            candidates = [
                config
                for config in self.llm_configs.values()
                if config.provider != LLMProvider.OLLAMA
            ]

        if not candidates:
            return RoutingDecision(
                selected_provider=LLMProvider.OLLAMA,  # fallback to Ollama anyway
                selected_model="qwen3-vl:8b",
                reasoning="ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì—†ìŒ - Ollamaë¡œ fallback",
                confidence=0.5,
                estimated_cost=0.0,
                estimated_latency=500,
                fallback_providers=[],
            )

        # ë¹„ìš©-ì„±ëŠ¥ ê· í˜•ìœ¼ë¡œ ì„ íƒ
        best_llm = self._select_best_llm(candidates, query, context)

        return RoutingDecision(
            selected_provider=best_llm.provider,
            selected_model=best_llm.model,
            reasoning=self._generate_reasoning(best_llm, quality, max_latency, max_cost),
            confidence=0.85,
            estimated_cost=self._estimate_cost(query, best_llm),
            estimated_latency=best_llm.latency_ms,
            fallback_providers=[c.provider for c in candidates if c != best_llm],
        )

    def _select_best_llm(
        self, candidates: list[LLMConfig], query: str, context: dict[str, Any]
    ) -> LLMConfig:
        """ìµœì  LLM ì„ íƒ ì•Œê³ ë¦¬ì¦˜"""
        # ì ìˆ˜ ê¸°ë°˜ ì„ íƒ (ë¹„ìš©, í’ˆì§ˆ, ì§€ì—° ì‹œê°„ ê· í˜•)
        scored_candidates = []

        for config in candidates:
            score = self._calculate_llm_score(config, query, context)
            scored_candidates.append((config, score))

        # ìµœê³  ì ìˆ˜ LLM ì„ íƒ
        return max(scored_candidates, key=lambda x: x[1])[0]

    def _calculate_llm_score(self, config: LLMConfig, query: str, context: dict[str, Any]) -> float:
        """LLM ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„)"""
        # í’ˆì§ˆ ì ìˆ˜ (0.4 ê°€ì¤‘ì¹˜)
        quality_score = {
            QualityTier.BASIC: 0.3,
            QualityTier.STANDARD: 0.6,
            QualityTier.PREMIUM: 0.8,
            QualityTier.ULTRA: 1.0,
        }[config.quality_tier]

        # ë¹„ìš© ì ìˆ˜ (ì—­ìˆ˜, 0.3 ê°€ì¤‘ì¹˜) - ì €ë¹„ìš©ì¼ìˆ˜ë¡ ë†’ìŒ
        cost_score = min(1.0, 1.0 / (config.cost_per_token * 1000 + 1))

        # ì§€ì—° ì‹œê°„ ì ìˆ˜ (ì—­ìˆ˜, 0.3 ê°€ì¤‘ì¹˜) - ë¹ ë¥¼ìˆ˜ë¡ ë†’ìŒ
        latency_score = min(1.0, 2000 / config.latency_ms)

        # ìµœì¢… ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        final_score = quality_score * 0.4 + cost_score * 0.3 + latency_score * 0.3

        return final_score

    def _generate_reasoning(
        self, config: LLMConfig, quality: QualityTier, max_latency: int, max_cost: float
    ) -> str:
        """ë¼ìš°íŒ… ì´ìœ  ìƒì„±"""
        reasons = []

        if config.quality_tier.value >= quality.value:
            reasons.append(f"í’ˆì§ˆ ìš”êµ¬ì‚¬í•­({quality.value}) ì¶©ì¡±")
        else:
            reasons.append(f"í’ˆì§ˆ ì œí•œìœ¼ë¡œ {config.quality_tier.value} ì„ íƒ")

        if config.latency_ms <= max_latency:
            reasons.append(f"ì§€ì—° ì‹œê°„({config.latency_ms}ms) í—ˆìš© ë²”ìœ„ ë‚´")
        else:
            reasons.append("ì§€ì—° ì‹œê°„ ì œí•œ ê³ ë ¤ë¨")

        if config.cost_per_token * 1000 <= max_cost:
            reasons.append("ë¹„ìš© ì˜ˆì‚° ë‚´")
        else:
            reasons.append("ìµœì  ë¹„ìš© ì„ íƒ")

        return f"{config.provider.value} ì„ íƒ: {', '.join(reasons)}"

    def _estimate_cost(self, query: str, config: LLMConfig) -> float:
        """ë¹„ìš© ì¶”ì • (ê·¼ì‚¬ì¹˜)"""
        # ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì • (ë¬¸ì ìˆ˜ ê¸°ë°˜)
        estimated_tokens = len(query) * 0.3  # ëŒ€ëµì ì¸ ë³€í™˜
        return estimated_tokens * config.cost_per_token

    async def execute_with_routing(
        self, query: str, context: dict[str, Any] | None = None
    ) -> dict[str, Any]:
        """
        ë¼ìš°íŒ… ê²°ì • í›„ ì‹¤ì œ ì‹¤í–‰
        """
        # Explicitly return dict[str, Any]
        result: dict[str, Any]
        """
        ë¼ìš°íŒ… ê²°ì • í›„ ì‹¤ì œ ì‹¤í–‰ (w/ Caching)
        """
        # 1. Check Cache
        cache_key = f"{query}::{context}"
        if cache_key in self._response_cache:
            entry = self._response_cache[cache_key]
            if time.time() - entry["timestamp"] < self._cache_ttl:
                # Move to end (LRU)
                self._response_cache.move_to_end(cache_key)
                logger.info("âš¡ï¸ Cache Hit! Returning cached response.")
                # Cast Any to expected dict type for MyPy
                cached_data: dict[str, Any] = entry["data"]
                return cached_data
            else:
                # Expired
                del self._response_cache[cache_key]

        routing_decision = self.route_request(query, context)

        # ë¼ìš°íŒ… ê¸°ë¡
        self.routing_history.append(
            {
                "timestamp": time.time(),
                "query": query[:100],  # ì¶•ì•½
                "decision": {
                    "provider": routing_decision.selected_provider.value,
                    "model": routing_decision.selected_model,
                    "reasoning": routing_decision.reasoning,
                    "confidence": routing_decision.confidence,
                },
            }
        )

        # ì‹¤ì œ LLM í˜¸ì¶œ (ì—¬ê¸°ì„œëŠ” ëª¨ì˜ êµ¬í˜„)
        try:
            response = await self._call_llm(routing_decision, query, context)
            response_data = {
                "success": True,
                "response": response,
                "routing": {
                    "provider": routing_decision.selected_provider.value,
                    "model": routing_decision.selected_model,
                    "reasoning": routing_decision.reasoning,
                    "estimated_cost": routing_decision.estimated_cost,
                    "estimated_latency": routing_decision.estimated_latency,
                },
            }

            # Cache Success Response
            self._response_cache[cache_key] = {"timestamp": time.time(), "data": response_data}
            if len(self._response_cache) > self._cache_max_size:
                self._response_cache.popitem(last=False)  # Remove oldest

            return response_data
        except Exception as e:
            # Fallback ì‹œë„
            if routing_decision.fallback_providers:
                return await self._try_fallback(
                    routing_decision.fallback_providers[0], query, context
                )
            else:
                return {"success": False, "error": str(e), "routing": routing_decision.__dict__}

    def _get_google_module(self) -> Any:
        """Helper for testability"""
        import google.generativeai as genai

        return genai

    async def _query_google(
        self, query: str, config: LLMConfig, context: dict[str, Any] | None
    ) -> str:
        """Google Gemini ì¿¼ë¦¬ ì‹¤í–‰ (ëª¨ë¸ Fallback ì ìš©)"""
        genai = self._get_google_module()

        # Phase 2-4: settings ì‚¬ìš©
        vault_client = None
        try:
            from AFO.security.vault_manager import vault as v1_client
            vault_client = v1_client
        except ImportError:
            try:
                # [ë§¹ì] ë“ë„ë‹¤ì¡° - ì§„ì‹¤ëœ ê²½ë¡œë¥¼ ì°¾ìœ¼ë©´ ë„ì›€ì´ ë”°ë¦„
                from security.vault_manager import vault as v2_client
                vault_client = v2_client
            except ImportError:
                vault_client = None

        if vault_client and config.api_key_env:
            api_key = vault_client.get_secret(config.api_key_env)
        else:
            # Fallback
            try:
                from AFO.config.settings import get_settings

                settings = get_settings()
                api_key = getattr(settings, config.api_key_env or "", None)
            except ImportError:
                api_key = os.getenv(config.api_key_env or "")

        if not api_key:
            raise ValueError(f"API Key not found for env var: {config.api_key_env}")

        genai.configure(api_key=api_key)

        # ì‹œë„í•  ëª¨ë¸ ëª©ë¡ (ì‹¤ì œ API ì¡°íšŒ ê²°ê³¼ ê¸°ë°˜)
        models_to_try = ["gemini-2.0-flash-exp", "gemini-flash-latest", "gemini-pro-latest"]
        last_error = None

        for model_name in models_to_try:
            try:
                model = genai.GenerativeModel(model_name)

                # ì•ˆì „ ì„¤ì • ì™„í™”
                safety_settings: list[dict[str, str]] = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]

                # contextê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì²˜ë¦¬
                context_dict = context or {}
                response = await model.generate_content_async(
                    query,
                    generation_config=genai.types.GenerationConfig(
                        temperature=context_dict.get("temperature", 0.7),
                        max_output_tokens=context_dict.get("max_tokens", 1000),
                    ),
                    safety_settings=safety_settings,
                )

                return str(response.text)
            except Exception as e:
                print(f"âš ï¸ Gemini ëª¨ë¸({model_name}) ì‹¤íŒ¨: {e}")
                last_error = e  # Update with the actual error
                continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„

        # ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ ì‹œ
        if last_error is not None:
            raise last_error
        raise RuntimeError("All Gemini models failed and no error was captured")

    async def _call_ollama(
        self, query: str, config: LLMConfig, context: dict[str, Any] | None = None
    ) -> str:
        """Ollama API í˜¸ì¶œ"""
        import httpx

        # Phase 2-4: settings ì‚¬ìš©
        if config.base_url:
            base_url = config.base_url
        else:
            try:
                from AFO.config.settings import get_settings

                base_url = get_settings().OLLAMA_BASE_URL
            except ImportError:
                try:
                    from config.settings import get_settings  # type: ignore

                    base_url = get_settings().OLLAMA_BASE_URL
                except ImportError:
                    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

        timeout_seconds = float(
            (context or {}).get("ollama_timeout_seconds", os.getenv("OLLAMA_TIMEOUT_SECONDS", "30"))
        )
        max_tokens = int((context or {}).get("max_tokens", config.max_tokens))
        temperature = float((context or {}).get("temperature", config.temperature))
        model = str((context or {}).get("ollama_model", config.model))
        num_ctx = int(
            (context or {}).get("ollama_num_ctx", getattr(config, "context_window", 4096))
        )
        num_threads = (context or {}).get("ollama_num_thread")

        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(timeout_seconds)) as client:
                options: dict[str, Any] = {
                    "temperature": temperature,
                    "num_predict": max_tokens,
                    "num_ctx": num_ctx,
                }
                if num_threads is not None:
                    with contextlib.suppress(Exception):
                        options["num_thread"] = int(num_threads)

                response = await client.post(
                    f"{base_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": query,
                        "stream": False,
                        "options": options,
                    },
                )
                response.raise_for_status()
                result = response.json()
                return str(result.get("response", ""))
        except Exception as e:
            logger.error(f"Ollama í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise

    async def _call_llm(
        self, decision: RoutingDecision, query: str, context: dict[str, Any] | None
    ) -> str:
        """ì‹¤ì œ LLM í˜¸ì¶œ"""
        provider = decision.selected_provider

        try:
            if provider == LLMProvider.OLLAMA:
                config = self.llm_configs.get(LLMProvider.OLLAMA)
                if config:
                    return await self._call_ollama(query, config, context)
                return "[Ollama Error] ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤."

            elif provider == LLMProvider.ANTHROPIC and API_WRAPPERS_AVAILABLE:
                if claude_api.is_available():
                    result = await claude_api.generate(query, max_tokens=1024)
                    if result.get("success"):
                        return str(result["content"])
                    else:
                        return f"[Claude Error] {result.get('error', 'Unknown error')}"
                else:
                    return "[Claude Unavailable] API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            elif provider == LLMProvider.GEMINI:
                # Gemini ì§ì ‘ í˜¸ì¶œ (Fallback ë¡œì§ í¬í•¨)
                config = self.llm_configs.get(LLMProvider.GEMINI)
                if config:
                    return await self._query_google(query, config, context)
                else:
                    return "[Gemini Unavailable] ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤."

            elif provider == LLMProvider.OPENAI and API_WRAPPERS_AVAILABLE:
                if openai_api.is_available():
                    result = await openai_api.generate(query, max_tokens=1024)
                    if result.get("success"):
                        return str(result["content"])
                    else:
                        return f"[OpenAI Error] {result.get('error', 'Unknown error')}"
                else:
                    return "[OpenAI Unavailable] API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

            else:
                # Fallback ëª¨ì˜ ì‘ë‹µ
                return f"[{provider.value}] {query}ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤. (API wrapper unavailable)"

        except Exception as e:
            logger.error(f"LLM í˜¸ì¶œ ì¤‘ ì˜ˆì™¸: {e}")
            raise

    async def _try_fallback(
        self, fallback_provider: LLMProvider, query: str, context: dict[str, Any] | None
    ) -> dict[str, Any]:
        """Fallback LLM ì‹œë„"""
        try:
            config = self.llm_configs.get(fallback_provider)
            if not config:
                raise ValueError(f"ì„¤ì •ë˜ì§€ ì•Šì€ provider: {fallback_provider}")

            # ê°„ë‹¨í•œ fallback í˜¸ì¶œ
            response = f"[Fallback {fallback_provider.value}] {query}ì— ëŒ€í•œ ì‘ë‹µì…ë‹ˆë‹¤."

            return {
                "success": True,
                "response": response,
                "routing": {
                    "provider": fallback_provider.value,
                    "model": config.model,
                    "reasoning": "Primary LLM ì‹¤íŒ¨ë¡œ fallback ì‚¬ìš©",
                    "is_fallback": True,
                },
            }
        except Exception as e:
            return {"success": False, "error": f"Fallbackë„ ì‹¤íŒ¨: {e!s}"}

    def get_routing_stats(self) -> dict[str, Any]:
        """ë¼ìš°íŒ… í†µê³„"""
        if not self.routing_history:
            return {"total_requests": 0}

        total_requests = len(self.routing_history)
        provider_usage: dict[str, Any] = {}

        for record in self.routing_history[-100:]:  # ìµœê·¼ 100ê°œ
            provider = record["decision"]["provider"]
            provider_usage[provider] = provider_usage.get(provider, 0) + 1

        return {
            "total_requests": total_requests,
            "provider_usage": provider_usage,
            "average_confidence": sum(
                r["decision"]["confidence"] for r in self.routing_history[-100:]
            )
            / min(100, len(self.routing_history)),
            "ollama_preference_ratio": provider_usage.get("ollama", 0) / total_requests
            if total_requests > 0
            else 0,
        }


# ê¸€ë¡œë²Œ LLMRouter ì¸ìŠ¤í„´ìŠ¤
llm_router = LLMRouter()


async def route_and_execute(query: str, context: dict[str, Any] | None = None) -> dict[str, Any]:
    """
    LLM ë¼ìš°íŒ… ë° ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤
    """
    return await llm_router.execute_with_routing(query, context)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    async def test_router() -> None:
        router = LLMRouter()

        test_queries = [
            "ê°„ë‹¨í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤",
            "ê³ í’ˆì§ˆ ë‹µë³€ì´ í•„ìš”í•œ ë³µì¡í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤",
            "ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤",
        ]

        print("ğŸ§­ LLM Router í…ŒìŠ¤íŠ¸")
        print("=" * 50)

        for query in test_queries:
            print(f"\nğŸ” ì¿¼ë¦¬: {query}")

            # ë¼ìš°íŒ… ê²°ì •
            decision = router.route_request(query)
            print(f"ğŸ“‹ ê²°ì •: {decision.selected_provider.value} ({decision.selected_model})")
            print(f"ğŸ’­ ì´ìœ : {decision.reasoning}")
            print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${decision.estimated_cost:.4f}")
            print(f"â±ï¸  ì˜ˆìƒ ì§€ì—°: {decision.estimated_latency}ms")
            # ì‹¤ì œ ì‹¤í–‰ (ëª¨ì˜)
            result = await router.execute_with_routing(query)
            print(f"âœ… ê²°ê³¼: {result['success']}")
            if result["success"]:
                print(f"ğŸ’¬ ì‘ë‹µ: {result['response'][:50]}...")

        # í†µê³„
        stats = router.get_routing_stats()
        print(f"\nğŸ“Š í†µê³„: {stats}")

    asyncio.run(test_router())
