# ğŸ« TICKET-013: vLLM TorchAO ê³ ì† ì„œë¹™ ì‹œìŠ¤í…œ êµ¬ì¶•

**ìš°ì„ ìˆœìœ„**: HIGH
**ìƒíƒœ**: PENDING
**ë‹´ë‹¹**: ìŠ¹ìƒ + AIíŒ€
**ì˜ì¡´ì„±**: TICKET-012 (TorchAO int8 ìµœì í™” ì™„ë£Œ í›„)
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 8ì‹œê°„

## ğŸ¯ ëª©í‘œ (Goal)

TorchAO int8 ìµœì í™”ëœ ëª¨ë¸ì„ vLLMìœ¼ë¡œ ê³ ì† ì„œë¹™í•˜ì—¬ ì™•êµ­ AI ì‹œìŠ¤í…œì˜ ì´ˆê³ ì† ì €ë¹„ìš© ì¶”ë¡  êµ¬í˜„.

## ğŸ“‹ ì‘ì—… ë‚´ìš©

### 1. vLLM ê²©ë¦¬ í™˜ê²½ êµ¬ì¶•
```python
# tools/vllm_torchao/pyproject.toml
[tool.poetry]
name = "vllm-torchao-serving"
version = "0.1.0"
description = "vLLM TorchAO ê³ ì† ì„œë¹™ ê²©ë¦¬ í™˜ê²½"

[tool.poetry.dependencies]
python = "^3.12,<3.14"
vllm = "^0.10.0"
torch = "^2.5.0"
torchao = "^0.7.0"
transformers = "^5.0.0rc1"
accelerate = "^1.0.0"
```

### 2. TorchAO ëª¨ë¸ vLLM ì„œë¹™ êµ¬í˜„
```python
# packages/afo-core/afo/vllm_torchao_serving.py
from vllm import EngineArgs, LLMEngine, SamplingParams
from transformers import TorchAoConfig
import torch

class VLLMTorchAOServing:
    def __init__(self, model_name: str, quantization: str = "torchao"):
        """TorchAO ëª¨ë¸ì„ vLLMìœ¼ë¡œ ì„œë¹™"""

        # TorchAO int8 ì„¤ì •
        torchao_config = TorchAoConfig("int8_weight_only")

        # vLLM ì—”ì§„ ì„¤ì •
        self.engine_args = EngineArgs(
            model=model_name,
            quantization=quantization,  # "torchao"
            dtype="auto",
            max_model_len=4096,
            gpu_memory_utilization=0.8,
            tensor_parallel_size=torch.cuda.device_count() if torch.cuda.is_available() else 1
        )

        self.engine = LLMEngine.from_engine_args(self.engine_args)
        self.tokenizer = self.engine.get_tokenizer()

    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.7):
        """ê³ ì† ì¶”ë¡  ì‹¤í–‰"""

        sampling_params = SamplingParams(
            temperature=temperature,
            max_tokens=max_tokens,
            stop=["\n\n", "###"]
        )

        # ë¹„ë™ê¸° ì¶”ë¡ 
        request_id = f"request_{hash(prompt)}"
        self.engine.add_request(request_id, prompt, sampling_params)

        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        while self.engine.has_unfinished_requests():
            step_outputs = self.engine.step()
            for output in step_outputs:
                if output.finished:
                    results.append(output)

        return self.tokenizer.decode(results[0].outputs[0].text) if results else ""
```

### 3. Trinity Score ê¸°ë°˜ ëª¨ë¸ ì„ íƒ ë° ì„œë¹™
```python
# packages/afo-core/afo/trinity_model_selector.py
from afo.vllm_torchao_serving import VLLMTorchAOServing
from afo.metrics import trinity_metric

class TrinityModelSelector:
    def __init__(self):
        self.models = {}  # model_name -> VLLMTorchAOServing instance
        self.trinity_scores = {}  # model_name -> trinity_score

    def register_model(self, model_name: str, trinity_score: float):
        """ëª¨ë¸ ë“±ë¡ ë° ì„œë¹™ ì¤€ë¹„"""
        if trinity_score >= 95.0:  # ê³ í’ˆì§ˆ ëª¨ë¸ë§Œ ë“±ë¡
            serving = VLLMTorchAOServing(model_name)
            self.models[model_name] = serving
            self.trinity_scores[model_name] = trinity_score

    def select_best_model(self, query_complexity: str = "medium"):
        """ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ"""

        if query_complexity == "low":
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ëŠ” ê°€ë²¼ìš´ ëª¨ë¸
            return max(self.trinity_scores.items(),
                      key=lambda x: x[1] if x[0].endswith("7B") else 0)

        elif query_complexity == "high":
            # ë³µì¡í•œ ì¿¼ë¦¬ëŠ” ê°•ë ¥í•œ ëª¨ë¸
            return max(self.trinity_scores.items(),
                      key=lambda x: x[1] if x[0].endswith("70B") else 0)

        else:
            # ì¤‘ê°„ ë³µì¡ë„ëŠ” Trinity Score ê¸°ë°˜ ì„ íƒ
            return max(self.trinity_scores.items(), key=lambda x: x[1])

    def generate_response(self, query: str, complexity: str = "medium"):
        """ìµœì  ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        best_model_name, _ = self.select_best_model(complexity)
        serving = self.models[best_model_name]

        response = serving.generate(query)
        return response, best_model_name
```

### 4. Chancellor Graph vLLM í†µí•©
```python
# Chancellor Graphì— vLLM ì„œë¹™ í†µí•©
from afo.trinity_model_selector import TrinityModelSelector

class ChancellorVLLMAgent:
    def __init__(self):
        self.model_selector = TrinityModelSelector()

        # TorchAO ìµœì í™”ëœ ëª¨ë¸ë“¤ ë“±ë¡
        torchao_models = [
            ("microsoft/DialoGPT-medium", 92.3),
            ("meta-llama/Llama-2-7b-chat-hf", 96.7),
            ("meta-llama/Llama-2-13b-chat-hf", 98.1),
        ]

        for model_name, score in torchao_models:
            self.model_selector.register_model(model_name, score)

    def query_kingdom(self, question: str, context_complexity: str = "medium"):
        """ì™•êµ­ ì§€ì‹ ì¿¼ë¦¬ ì²˜ë¦¬"""

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë³µì¡ë„ ë¶„ì„
        complexity = self.analyze_complexity(question)

        # ìµœì  ëª¨ë¸ ì„ íƒ ë° ì¶”ë¡ 
        response, model_used = self.model_selector.generate_response(
            question, complexity
        )

        return {
            "response": response,
            "model_used": model_used,
            "trinity_score": self.model_selector.trinity_scores[model_used],
            "inference_time": "measured_time"  # ì‹¤ì œ ì¸¡ì • ê°’
        }

    def analyze_complexity(self, question: str):
        """ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„"""
        if len(question.split()) < 10:
            return "low"
        elif len(question.split()) > 50 or any(word in question.lower()
              for word in ["analyze", "compare", "explain", "design"]):
            return "high"
        else:
            return "medium"
```

### 5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
```python
# packages/afo-core/afo/vllm_performance_monitor.py
import time
import psutil
from afo.vllm_torchao_serving import VLLMTorchAOServing

class VLLMPerformanceMonitor:
    def __init__(self, serving: VLLMTorchAOServing):
        self.serving = serving
        self.metrics = {
            "throughput": [],
            "latency": [],
            "memory_usage": [],
            "gpu_utilization": []
        }

    def benchmark_model(self, test_queries: list, batch_size: int = 1):
        """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""

        for query in test_queries:
            start_time = time.time()

            # ì¶”ë¡  ì‹¤í–‰
            response = self.serving.generate(query)

            end_time = time.time()

            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            latency = end_time - start_time
            memory_mb = psutil.virtual_memory().used / 1024 / 1024
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0

            self.metrics["latency"].append(latency)
            self.metrics["memory_usage"].append(memory_mb)
            self.metrics["gpu_memory"].append(gpu_memory)

        # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_latency = sum(self.metrics["latency"]) / len(self.metrics["latency"])
        avg_memory = sum(self.metrics["memory_usage"]) / len(self.metrics["memory_usage"])
        avg_gpu_memory = sum(self.metrics["gpu_memory"]) / len(self.metrics["gpu_memory"])

        throughput = len(test_queries) / sum(self.metrics["latency"])

        return {
            "throughput_qps": throughput,
            "avg_latency_ms": avg_latency * 1000,
            "avg_memory_mb": avg_memory,
            "avg_gpu_memory_mb": avg_gpu_memory
        }
```

## âœ… Acceptance Criteria

- [ ] vLLM ê²©ë¦¬ í™˜ê²½ êµ¬ì¶• ë° TorchAO í†µí•© ì„±ê³µ
- [ ] TorchAO int8 ëª¨ë¸ vLLM ì„œë¹™ êµ¬í˜„ ì™„ë£Œ
- [ ] Trinity Score ê¸°ë°˜ ëª¨ë¸ ì„ íƒ ì‹œìŠ¤í…œ ì™„ì„±
- [ ] Chancellor Graph vLLM í†µí•© ì ìš©
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë° ìµœì í™” ì™„ë£Œ (throughput 1.5xâ†‘ ëª©í‘œ)
- [ ] ê³ ì† ì„œë¹™ API ì—”ë“œí¬ì¸íŠ¸ êµ¬ì¶•

## ğŸ”’ ì œì•½ì‚¬í•­

- **LOCKED**: antigravity-seal-2025-12-30 ê´€ë ¨ íŒŒì¼ ì ˆëŒ€ ìˆ˜ì • ê¸ˆì§€
- **ì•ˆì „ ìš°ì„ **: ê²©ë¦¬ í™˜ê²½ì—ì„œ ì¶©ë¶„íˆ í…ŒìŠ¤íŠ¸ í›„ ë©”ì¸ ì ìš©
- **GPU ë©”ëª¨ë¦¬**: ìµœëŒ€ 80% í™œìš©, OOM ë°©ì§€

## ğŸš¨ ë¦¬ìŠ¤í¬ ë° ì™„í™”

| ë¦¬ìŠ¤í¬ | í™•ë¥  | ì˜í–¥ | ì™„í™” ë°©ì•ˆ |
|--------|------|------|-----------|
| vLLM TorchAO í†µí•© ë³µì¡ì„± | ì¤‘ê°„ | ì¤‘ê°„ | ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸ + ê³µì‹ ë¬¸ì„œ ì¤€ìˆ˜ |
| GPU ë©”ëª¨ë¦¬ ë¶€ì¡± | ë†’ìŒ | ë†’ìŒ | ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ + fallback ëª¨ë¸ |
| Throughput ëª©í‘œ ë¯¸ë‹¬ | ë‚®ìŒ | ì¤‘ê°„ | ë²¤ì¹˜ë§ˆí‚¹ ê¸°ë°˜ íŠœë‹ |

## ğŸ”„ ë¡¤ë°± ê³„íš

1. vLLM ì„œë¹™ ì¤‘ë‹¨ â†’ í‘œì¤€ Transformers ì¶”ë¡ 
2. TorchAO ëª¨ë¸ í•´ì œ â†’ FP16 ëª¨ë¸ ì‚¬ìš©
3. ê³ ì† ì„œë¹™ API ì œê±° â†’ ê¸°ë³¸ API ë³µì›

## ğŸ“Š Trinity Score ì˜í–¥

- **çœ (Truth)**: +9 (native ì •í™• ë¡œë“œ + PagedAttention)
- **å–„ (Goodness)**: +9 (throughput 1.5~2xâ†‘ + ë©”ëª¨ë¦¬ 50~70%â†“)
- **ç¾ (Beauty)**: +9 (TorchAO backend + vLLM API ìš°ì•„í•¨)
- **å­ (Serenity)**: +8 (í˜•ë‹˜ ê³ ì† ì„œë¹™ ìš©ì´ì„±)
- **æ°¸ (Eternity)**: +9 (PyTorch native + vLLM ì¥ê¸° ì§€ì›)

**ì˜ˆìƒ ì´ì **: 78.3 â†’ **100.0** (ê³ ì† ì €ë¹„ìš© ê¶ê·¹ ë‹¬ì„±)

## ğŸ“ ì‘ì—… ë¡œê·¸

- **ì‹œì‘ì¼**: 2025-12-31 (TorchAO ìµœì í™” ì™„ë£Œ í›„)
- **ì™„ë£Œì¼**: ì˜ˆì •
- **ì‹¤ì œ ì†Œìš”ì‹œê°„**: ì˜ˆì •

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- `tools/transformers_v5/` - TorchAO ê²©ë¦¬ í™˜ê²½
- `packages/afo-core/afo/vllm_torchao_serving.py` - vLLM ì„œë¹™ êµ¬í˜„
- `packages/afo-core/afo/trinity_model_selector.py` - Trinity ê¸°ë°˜ ì„ íƒ
- `packages/afo-core/afo/vllm_performance_monitor.py` - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
